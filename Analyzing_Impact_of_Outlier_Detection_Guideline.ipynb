{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgBQTFO1+uX2cVZvFQZPEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dajley/-Analyzing-Impact-of-Outlier-Detection-Guideline/blob/main/Analyzing_Impact_of_Outlier_Detection_Guideline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying the distribution of your data** is a key step in exploratory data analysis (EDA), especially when you're preparing for outlier detection, feature engineering, or model selection. Identifying the distribution of your data involves understanding the underlying shape and characteristics of its frequency distribution."
      ],
      "metadata": {
        "id": "ozo7Gxv9PQT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Identify Data's Distribution:\n",
        "\n",
        "*   Descriptive Statistics\n",
        "*   Visualization Techniques\n",
        "*   Interpretation\n",
        "*   Statistical Tests\n",
        "*   Comparisons and Considerations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YScDtDKUPT62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For agriculteral dataset giving crop yield based off certain features:\n",
        "\n",
        "\n",
        "*   Understand each feature (Catergorical variables vs. Numberical variables and Target variable)\n",
        "*   Explore Numerical Distributions\n",
        "*   Check for Skewed Distributions\n",
        "*   Use Boxplot for Outlier Detection\n",
        "*   Visualize Yield by Category\n",
        "*   Check for Relationships Between Inputs\n",
        "*   Modeling Implication Tips\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jiwKc71zSDIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect outliers (using IQR or Z-score)**\n",
        "Flag the outliers but don’t remove them yet — instead, keep track of which rows have outliers.\n",
        "\n",
        "**Create three versions of the dataset**\n",
        "*   Dataset A (Remove outliers): Drop rows with outliers.\n",
        "*   Dataset B (Median Imputation): Replace outlier values with the median of that feature.\n",
        "*   Dataset C (Mean Imputation): Replace outlier values with the mean of that feature.\n",
        "\n",
        "**Train ML models on each dataset**\n",
        "Split each dataset into train and test sets (same random seed), train the same ML model (e.g., Random Forest), and evaluate performance.\n",
        "\n",
        "**Compare results**\n",
        "Evaluate RMSE, MAE, and R² for each dataset and analyze which outlier handling method works best."
      ],
      "metadata": {
        "id": "2qfmnbNlcLbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('crop_yield_data.csv')\n",
        "X = df.drop(columns=['crop_yield'])\n",
        "y = df['crop_yield']\n",
        "\n",
        "# --- 1. Detect Outliers using IQR ---\n",
        "Q1 = X.quantile(0.25)\n",
        "Q3 = X.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Create a mask where True means the value is an outlier\n",
        "outlier_mask = ((X < lower_bound) | (X > upper_bound))\n",
        "\n",
        "# --- 2. Create datasets ---\n",
        "\n",
        "# Dataset A: Remove rows with any outlier\n",
        "rows_to_remove = outlier_mask.any(axis=1)\n",
        "X_remove = X.loc[~rows_to_remove]\n",
        "y_remove = y.loc[~rows_to_remove]\n",
        "\n",
        "# Dataset B: Replace outliers with median\n",
        "X_median = X.copy()\n",
        "for col in X.columns:\n",
        "    median_val = X[col].median()\n",
        "    X_median.loc[outlier_mask[col], col] = median_val\n",
        "\n",
        "# Dataset C: Replace outliers with mean\n",
        "X_mean = X.copy()\n",
        "for col in X.columns:\n",
        "    mean_val = X[col].mean()\n",
        "    X_mean.loc[outlier_mask[col], col] = mean_val\n",
        "\n",
        "# --- 3. Train-test split (same random state for comparability) ---\n",
        "def split_data(X, y):\n",
        "    return train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = split_data(X, y)\n",
        "X_train_remove, X_test_remove, y_train_remove, y_test_remove = split_data(X_remove, y_remove)\n",
        "X_train_median, X_test_median, y_train_median, y_test_median = split_data(X_median, y)\n",
        "X_train_mean, X_test_mean, y_train_mean, y_test_mean = split_data(X_mean, y)\n",
        "\n",
        "# --- 4. Train and evaluate ---\n",
        "def train_evaluate(X_train, y_train, X_test, y_test):\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "results = {}\n",
        "results['Original'] = train_evaluate(X_train_orig, y_train_orig, X_test_orig, y_test_orig)\n",
        "results['Remove Outliers'] = train_evaluate(X_train_remove, y_train_remove, X_test_remove, y_test_remove)\n",
        "results['Median Imputation'] = train_evaluate(X_train_median, y_train_median, X_test_median, y_test_median)\n",
        "results['Mean Imputation'] = train_evaluate(X_train_mean, y_train_mean, X_test_mean, y_test_mean)\n",
        "\n",
        "# --- 5. Print results ---\n",
        "for method, metrics in results.items():\n",
        "    print(f\"{method}: RMSE={metrics[0]:.3f}, MAE={metrics[1]:.3f}, R2={metrics[2]:.3f}\")\n"
      ],
      "metadata": {
        "id": "iYPYJJqBQEqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obsFGQfExk2B"
      },
      "outputs": [],
      "source": []
    }
  ]
}